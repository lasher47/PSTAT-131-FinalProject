---
title: "Predicting the Winner of League of Legends"
author: "William Long"
date: "Fall 2022"
output:
  html_document:
    toc: true
    toc_float: true
    df_print: paged
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

The goal of this project is to generate a machine learning model that can predict the eventual winner of a ranked game of League of Legends using numeric data from resources and objectives taken from the first 10 minutes of that game.

### What is League of Legends?

League of Legends(LoL) is a MOBA(Mobile Online Battle Arena) game created by Riot Games where two teams of 5 champions each versus each other in a tower defense-esque map with 3 lanes. The game revolves heavily around strategic resource generation and control of those resources to make your team's champions stronger and more capable of destroying the enemy team's base. A team wins by destroying the other team's nexus(the core base structure) first.

![Map of Summoner's Rift with labels](images/leagueMap.png)

### Why this model might be useful

In League of Legends, once your team gets stronger from early resource advantages, it becomes even easier for your team to fight for more resources. As such, early game advantages often snowball exponentially into an eventual victory for the leading team. This model will attempt to predict the eventual winner of a game given this early game data. Hopefully this will give other players insight as to when its statistically worth it to surrender after a bad early game and on what resources to prioritize in the early game.

### Loading Packages and Raw Data

```{r packages, warning = FALSE, echo = FALSE, results = "hide"}
library(dplyr) #Basic R functions
library(tidymodels)  #Tidymodels and tidyverse for the core modeling framework
library(tidyverse)
library(janitor)   #Cleaning data
library(yardstick)   #Model metrics
library(ggplot2)   #Graphs and visuals
library(vip)     #Variable importance
library(corrr)   #Correlation
library(corrplot)
library(ranger)



```

The data that we will be using for this project is a Kaggle dataset(<https://www.kaggle.com/datasets/bobbyscience/league-of-legends-diamond-ranked-games-10-min>) containing early game data from high elo(D1 - Masters) ranked games from the EUW server. The dataset encompasses approximately 10,000 games with variables tracking statistics from both teams snapshotted at the 10:00 minute mark. Descriptions of every variable are available in the codebook in this project repository.

```{r read}
#Reading in raw csv file
league <- read.csv("data/raw/high_diamond_ranked_10min.csv")

```

## Pre-Processing and EDA

### Converting some variables into categorical

All of the variables in our raw dataset are numeric by default, but some of them make more sense to represent as factor variables. `blueWins` is an obvious one to change, but some others such as `blueDragons` ,which tracks how many dragons the Blue team has slain, makes more sense to treat as a factor variable because it is only possible to kill at most 1 dragon before 10:00 of game time.

```{r}

league_clean <- league

#Blue team categorical variables
league_clean$blueWins <- as.factor(league_clean$blueWins)
league_clean$blueDragons <- as.factor(league_clean$blueDragons)
league_clean$blueHeralds <- as.factor(league_clean$blueHeralds)
league_clean$blueFirstBlood <- as.factor(league_clean$blueFirstBlood)

#Red team categorical variables
league_clean$redDragons <- as.factor(league_clean$redDragons)
league_clean$redHeralds <- as.factor(league_clean$redHeralds)

```

We will also check for any missing data in our dataset. 


```{r missing}
sum(is.na(league_clean))
```

As expected, there is no missing data here because this was all sourced from Riot Games' official API which only records completed games.

```{r response}
#Summing up all the times the Blue Team wins over 9879 ranked games
plot(league_clean$blueWins)
summary(league_clean$blueWins)

```

Since there is no missing data in this dataset, we can conclude that the sample winrate for blue team is 4930/9879 = 0.499. This is actually lower than blue side winrate for most competitive professional leagues because blue side has easier access to neutral objectives(dragons, heralds, and Baron Nashor), first champion pick priority, and a slightly better camera angle. Having our sample winrate be almost exactly 50% makes it easier for our metrics to properly evaluate the performance of our model.

### Cleaning Data

On first glance, we can see that some variables in this dataset are actually not useful for a machine learning model because they are derived purely from information from other variables. For example, `blueGoldDiff` represents `blueTotalGold` - `redTotalGold`  at 10 min, but this variable doesn't provide our model with any extra information. In a similar vein, there are also redundant variables for Red team that represent information that is already conveyed by a similar Blue team variable. `redFirstBlood` is a binary variable that represents whether or not the Red team secured the first champion kill of the game, but `blueFirstBlood` represents the same information because only one team can get the first champion kill. The code chunk below drops excess variables that are already 100% correlated with other variables.

```{r excess}

league_clean <- league_clean %>%
  select(-c(redFirstBlood, redExperienceDiff, blueExperienceDiff, redGoldDiff, blueGoldDiff)) #Redundant difference variables and redFirstBlood

league_clean <- league_clean %>%
  select(-c(blueEliteMonsters, redEliteMonsters))
# Both "EliteMonsters" variables are also just the sum of heralds and dragons and can be dropped

league_clean <- league_clean %>%
  select(-c(blueCSPerMin, redCSPerMin, blueGoldPerMin, redGoldPerMin))
#Dropping the "PerMin" variables as those are just derived from the variables that represent the total/10 minutes. 

cor.test(league_clean$blueKills, league_clean$redDeaths)  
cor.test(league_clean$blueDeaths, league_clean$redKills)   #These 4 variables are all directly correlated with each other. Makes sense intuitively because a kill for one team would equal a death for the other team. We will drop the red ones below.

league_clean <- league_clean %>%
  select(-c(redDeaths, redKills))

#From my previous knowledge of the game, I know that total minion kills and jungle minion kills are really just resources to farm for gold and experience. As such, their impact on the game is already reflected in those other stats.

league_clean <- league_clean %>%
  select(-c(blueTotalMinionsKilled, blueTotalJungleMinionsKilled, redTotalMinionsKilled, redTotalJungleMinionsKilled))

#In a similar vein, a team's total experience is just a way to gauge their average level which is what actually affects a champion's combat strength. As such, we can remove the "TotalExperience" variables from both teams as well. 

league_clean <- league_clean %>%
  select(-c(blueTotalExperience, redTotalExperience))
  
```

### Correlations between variables

```{r corr}
#Testing correlations between numeric variables
league_clean %>%
    select(is.numeric) %>% 
    select(-c(gameId)) %>%  
    cor() %>% 
    corrplot(type = 'full', diag = TRUE,  
           method = 'circle', col = COL2("RdBu"))
```

Most of our numeric variables have reasonable and intuitive correlations. For example, `blueKills` is obviously correlated with `blueAssists`, but it is even more correlated with `blueTotalGold`. Kills are worth 300 gold by default, with first blood being worth 400 gold. Securing kills is actually one of the fastest ways to snowball as a champion kill by itself is worth about 15 minion kills. Killing a lane opponent also lets you farm minions without being pressured, gives you an opportunity to obtain even more gold from turret plates, and denies experience to your opponents since they aren't around to soak up minion experience.

The ward and tower related variables have suprisingly low correlation with any other numeric variables. Wards are items you can place in-game that provide vision over a certain area for a fixed amount of time. Normally the game hides most of the map in a fog of war, such that you can only see the immediate area around your champion and your allies. Wards grant your team a limited amount of vision elsewhere which helps to detect flanks or lurking enemies. However just because a ward is placed does not mean that that ward is actually useful. Perhaps it is in an irrelevant spot that no one on the map will frequent. Wards also do not automatically alert you to enemies that they spot, you still have to manually check your mini-map.



![Ward giving vision of a bush over the wall](images/wardOverWall.jpg)

From past experience, I know that is actually rare for a tower to fall by 10:00. There are 3 towers per lane, referred to as T1, T2, and T3 towers respectively. T1 towers are the outermost towers and thus, the first to fall because for each lane, you have to destroy the preceding tower before sieging the next. Destroying towers, especially in the early game where you can get the full gold value of the tower plates, also grants a significant amount of gold which is why it seems very odd that there's only a minor correlation to a team's gold and gold differential. One hypothesis that I have is that destroying a tower so early in the game actually makes it easier for the team that lost the tower to safely farm back the deficit. Once a lane has lost their T1 tower, the minions in that lane will naturally push further towards the enemy base and forcing your team's champions to have to overextend to farm the minions in that lane. In this case, destroying an enemy tower gives your team an immediate economic reward, but puts your team at a positional disadvantage.


I also want to see if there's any correlation between towers destroyed and heralds slain by each team. A rift herald is a neutral objective in League of Legends that can be slain by either team. Once slain, the team that killed the herald can re-summon it again to cause it ram into an enemy tower, dealing a large amount of damage to the tower. This makes it much easier for a team with a herald to siege towers and I hypothesize that teams that have slain a rift herald before 10 min will have destroyed more towers on average. Below are summary statistics for `blueTowersDestroyed` and `redTowersDestroyed` grouped by `blueHeralds` and `redHeralds` respectively. 

![Rift herald hitting tower](images/riftHeraldBonk.webp)

```{r}
tapply(league_clean$blueTowersDestroyed, league_clean$blueHeralds, summary)

tapply(league_clean$redTowersDestroyed, league_clean$redHeralds, summary)

#Herald variables range from 0-1

```

My hypothesis seems to be correct on this. A team with a herald destroys 0.16 towers by 10 minutes on average, compared to a team without a herald which only destroys 0.02 towers by 10 minutes on average.

### Warding Correlation with Victory

To determine if warding in the early game is actually correlated at all with winning, we can model a simplified logistic regression recipe and gauge the accuracy of that model.

```{r wards}
set.seed(477)
league_clean_split <- initial_split(league_clean, prop = 0.8, strata = blueWins)

league_train <- training(league_clean_split)
league_test <- testing(league_clean_split)

league_recipe <- recipe(blueWins ~ blueWardsPlaced + blueWardsDestroyed + redWardsPlaced + redWardsDestroyed, data = league_train)
  
simple_model <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

simple_wf <- workflow() %>%
  add_model(simple_model) %>%
  add_recipe(league_recipe)

simple_fit <- fit(simple_wf, league_train)

simple_train_res <- predict(simple_fit, new_data = league_train %>% dplyr::select(-blueWins))
simple_train_res <- bind_cols(simple_train_res, league_train %>% dplyr::select(blueWins))

accuracy(simple_train_res, truth = blueWins, estimate = .pred_class)




```

Our accuracy from this simplified model is only about 0.54, marginally better than a coin flip. To better explore the properties of the ward variables, let's plot some boxplots of them to get a sense of the distribution.

```{r}
ggplot(league_clean, aes(blueWardsPlaced)) +  
  geom_boxplot(fill = "#04c5e7")

ggplot(league_clean, aes(redWardsPlaced)) +  
  geom_boxplot(fill = "#fb0410")

ggplot(league_clean, aes(blueWardsDestroyed)) +
  geom_boxplot(fill = "#04c5e7")      #

ggplot(league_clean, aes(redWardsDestroyed)) +  
  geom_boxplot(fill = "#fb0410")

summary(league_clean$blueWardsPlaced)
summary(league_clean$redWardsPlaced)

```

From the boxplots and summaries above, we can see that the mean of wards placed for both teams by 10:00 is about 22 wards, but there are a ton of outlier values. Some games have teams place over 200 wards! That's more than 20 wards/min which far exceeds any reasonable pace for wards. In League of Legends, there are two types of wards: free, temporary wards that go on cooldown for about 120 seconds every time you place them and control wards which are permanent wards that cost 75 gold to buy. You can always place a control ward as long as you have one bought in your inventory, but you can only have 1 control ward per champion on the map at the same time. Since the frequency of free wards are gated by cooldown, the only way to achieve these extreme outlier values mentioned above is to buy an absurd amount of control wards. Way more than any normal gameplay would require and likely an excessive drain on gold, which may help to explain why ward placement doesn't seem to correlate well with winning. More strategic ward placement may still correlate with winning but the variables in this data set only give the raw quantity of wards and do not distinguish between the quality of the wards. As such, we will be dropping the ward variables from our model as well.

```{r wardDrop}
league_clean <- league_clean %>%
  select(-c(redWardsPlaced, redWardsDestroyed, blueWardsPlaced, blueWardsDestroyed))
```

```{r, eval=FALSE}
#Saving our cleaned dataset to data/processed
write.csv(league_clean, "data/processed/league_clean.csv")
```

## Data Splitting and Cross-validation

We will start splitting our data into training and testing splits, with an initial split of 80/20 training/testing and stratified on our outcome variable.

```{r split}
#Overriding previous variables from the simple model for wards from above.

set.seed(526)
league_clean_split <- initial_split(league_clean, prop = 0.8, strata = blueWins)

league_train <- training(league_clean_split)
league_test <- testing(league_clean_split)

dim(league_train)
dim(league_test)

```

After our split, we have 7903 observations in our training data and 1976 observations in our testing data.

We will also use K-fold cross validation to improve our model's performance with unseen data.

```{r}
league_folds <- vfold_cv(league_train, v=5,strata = blueWins, repeats = 5) #5 fold CV with 5 repeats. 
```

## Model building

Now we're ready to start building our full models to try to predict whether Blue team or Red Team wins.

### Building the Recipe

We'll start by setting up an initial recipe and workflow below. We'll use every variable left in our cleaned dataset as predictors with the exception of **gameID** and **blueWins**. Interactions terms are also included with reasoning supported from our EDA. 

```{r recipe}
#Overriding previous recipe from simple model

league_recipe <- recipe(blueWins ~ blueFirstBlood + blueKills + blueDeaths + blueAssists + blueDragons + blueHeralds + blueTowersDestroyed + blueTotalGold + blueAvgLevel + redAssists + redDragons + redHeralds + redTowersDestroyed + redTotalGold + redAvgLevel, data = league_train) %>%
  step_dummy(all_nominal_predictors()) %>% 
  step_interact(terms = ~ blueKills:blueAssists:blueTotalGold:redAvgLevel:blueAvgLevel + blueDeaths:redAssists:redTotalGold:blueAvgLevel:redAvgLevel + starts_with("blueHerald"):blueTowersDestroyed + starts_with("redHerald"):redTowersDestroyed) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors())  #Centering and scaling our predictors

#Numeric variable interactions were based on high correlation values from our correlation plot above

#Heralds and towers interactions were based on our summary statics for those above

```

### Logistic Regression

Logistic Regression done below with tuning parameters  evaluated over our folds.

```{r, eval = FALSE}
log_model <- logistic_reg(mode = "classification", engine = "glmnet", penalty = tune(), mixture = tune())

log_wkflow <- workflow() %>%
  add_recipe(league_recipe) %>%
  add_model(log_model)

log_grid <- grid_regular(penalty(range = c(-5,5)), mixture(range = c(0,1)), levels = 10)

log_results <- tune_grid(log_wkflow, resamples = league_folds, grid =  log_grid)
  
```
Our accuracy for this type of model seems to peak around 70% and our `roc_auc` seems to peak around 0.8

```{r, eval = FALSE}
#Saving our results so we don't have to run the model again
save(log_results, file = "results/log_results.rda")
```

```{r}
load(file = "results/log_results.rda")
autoplot(log_results)
```



### Decision Tree

We will also fit decision tree model with tuning parameters of `cost_complexity`, `tree_depth`, and `min_n`. 

```{r, eval = FALSE}
class_tree_model <- decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("classification") %>%
  set_args(cost_complexity = tune()) %>%
  set_args(tree_depth = tune())
  
class_tree_wf <- workflow() %>%
  add_model(class_tree_model) %>%
  add_recipe(league_recipe)

class_tree_grid <- grid_regular(cost_complexity(range = c(-3, -1)), tree_depth(), levels = 5)

class_tree_res <- tune_grid(
  class_tree_wf, 
  resamples = league_folds, 
  grid = class_tree_grid, 
  metrics = metric_set(roc_auc, accuracy)
)

```
Accuracy peaks around the same value as logistic regression, but `roc_auc` has slightly decreased.

```{r, eval = FALSE}
save(class_tree_res, file = "results/class_tree_results.rda")
```

```{r}
load(file = "results/class_tree_results.rda")
autoplot(class_tree_res)
```


### Random Forest model

Here we will also fit a random forest model with tuning parameters of `mtry`, `trees`, and `min_n`. `mtry` is the number of predictors randomly sampled per split and I have the max value set to just slightly below our total sum of predictors.`min_n` is  the mininum number of observations needed for our model to create another split and this parameter is set to a range of 50-500. `trees` is set to a range of 64-128 which is a good compromise between training performance and processing power. Even so, this model still took approximately 6 hours to fit. 
```{r rf, eval = FALSE}
rf_model <- rand_forest() %>%
  set_engine("ranger") %>% 
  set_mode("classification")

rf_wf <- workflow() %>%
  add_model(rf_model %>% set_args(mtry = tune(), trees = tune(), min_n = tune())) %>%
  add_recipe(league_recipe)

rf_param_grid <- grid_regular(mtry(range=c(1,14)), trees(range=c(64,128)), min_n(range = c(50, 500)), levels = 5)
rf_res <- tune_grid(
  rf_wf, 
  resamples = league_folds, 
  grid = rf_param_grid, 
  metrics = metric_set(accuracy, roc_auc)
)
```

```{r, eval = FALSE}
#Definitely don't want to have to refit this model
save(rf_res, file = "results/rf_res.rda")

```

```{r}
load(file = "results/rf_res.rda")
autoplot(rf_res)

```

### Boosted Tree

Our last model to evaluate will be a boosted tree with tuning parameters of `trees`, `mtry`, and `learn_rate`. 

```{r, eval = FALSE}
boost_model <- boost_tree() %>%
  set_engine("xgboost") %>%
  set_mode("classification") %>%
  set_args(trees = tune()) %>%
  set_args(mtry = tune()) %>%
  set_args(learn_rate = tune())

boost_wf <- workflow() %>%
  add_model(boost_model) %>%
  add_recipe(league_recipe)

boost_grid <- grid_regular(trees(range(c(10, 2000))), mtry(range(c(1, 14))), learn_rate(range(c(0.1, 0.3))))

boost_tree_res <- tune_grid(
  boost_wf, 
  resamples = league_folds, 
  grid = boost_grid, 
  metrics = metric_set(roc_auc, accuracy)
)
```
```{r, eval = FALSE}
#Saving results of model 
save(boost_tree_res, file = "results/boost_res.rda")
```


```{r}
load(file = "results/boost_res.rda")
autoplot(boost_tree_res)
```

## Model Selection

### Fitting onto Testing Data

## Conclusion

### Variable importance to winning 
